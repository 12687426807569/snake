{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbd2163"
      },
      "source": [
        "%pip install tensorflow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8a6f6b6"
      },
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Keep the global variables as defined by the user\n",
        "board_size = 4\n",
        "max_steps = 2 * board_size * board_size\n",
        "\n",
        "class SnakeEnv:\n",
        "    def __init__(self, board_size, max_steps):  # Use the parameters passed during initialization\n",
        "        self.board_size = board_size\n",
        "        self.max_steps = max_steps\n",
        "        self.snake = collections.deque()\n",
        "        self.fruit_pos = None\n",
        "        self.steps = 0\n",
        "        self.board = np.zeros((board_size, board_size), dtype=int)  # Add board back\n",
        "        self.game_over = False\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Initialize the game state\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)  # Use self.board_size\n",
        "        self.snake = collections.deque([(0, 0)])\n",
        "        self.fruit_pos = self._generate_fruit()\n",
        "        self.steps = 0\n",
        "        self.game_over = False\n",
        "        self.board[self.snake[0]] = 1\n",
        "        # Return the state index directly\n",
        "        return self._get_state_index()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Take an action and return the next state, reward, and done flag\n",
        "        if self.game_over:\n",
        "            # Return the current state index when game is over\n",
        "            return self._get_state_index(), 0, True, {}\n",
        "\n",
        "        self.steps += 1\n",
        "\n",
        "        # Calculate next head position based on action (0: up, 1: down, 2: left, 3: right)\n",
        "        head_r, head_c = self.snake[0]  # Access snake head - check if deque is not empty\n",
        "        if action == 0:  # Up\n",
        "            next_head = (head_r - 1, head_c)\n",
        "        elif action == 1:  # Down\n",
        "            next_head = (head_r + 1, head_c)\n",
        "        elif action == 2:  # Left\n",
        "            next_head = (head_r, head_c - 1)\n",
        "        elif action == 3:  # Right\n",
        "            next_head = (head_r, head_c + 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Check for collisions *before* modifying the snake deque\n",
        "        collision = self._is_collision(next_head)\n",
        "\n",
        "        # Calculate previous distance to fruit\n",
        "        prev_dist = abs(head_r - self.fruit_pos[0]) + abs(head_c - self.fruit_pos[1]) if self.fruit_pos else 0\n",
        "\n",
        "        ate_fruit = False\n",
        "        if next_head == self.fruit_pos:\n",
        "            ate_fruit = True\n",
        "            self.fruit_pos = self._generate_fruit()\n",
        "            self.steps = 0  # Reset steps on eating fruit\n",
        "            # If fruit is eaten, add the new head but do not remove the tail\n",
        "            self.snake.appendleft(next_head)\n",
        "        else:\n",
        "            # If no fruit is eaten, remove the tail and add the new head\n",
        "            # Ensure snake is not empty before removing tail\n",
        "            if len(self.snake) > 0:\n",
        "                self.board[self.snake[-1]] = 0  # Remove tail from board\n",
        "                self.snake.pop()  # Remove tail\n",
        "            # Add new head if no collision\n",
        "            if not collision:\n",
        "                self.snake.appendleft(next_head)\n",
        "\n",
        "        # Update board representation (only for head and potentially old tail/new fruit)\n",
        "        if not collision:  # Only update board if no collision\n",
        "            self.board[next_head] = 1  # Add new head\n",
        "            if ate_fruit and self.fruit_pos:\n",
        "                self.board[self.fruit_pos[0], self.fruit_pos[1]] = 3  # Add new fruit to board\n",
        "\n",
        "        # Calculate current distance to fruit\n",
        "        current_dist = abs(next_head[0] - self.fruit_pos[0]) + abs(next_head[1] - self.fruit_pos[1]) if self.fruit_pos else 0\n",
        "\n",
        "        reward = self._calculate_reward(prev_dist, current_dist, ate_fruit, collision)\n",
        "\n",
        "        # Check game over conditions *after* calculating reward\n",
        "        if collision or self.steps >= self.max_steps or self.fruit_pos is None: # Added check for fruit_pos is None\n",
        "            self.game_over = True\n",
        "            if collision or self.steps >= self.max_steps:\n",
        "                 reward = -10 # Collision or max steps reached penalty\n",
        "            elif self.fruit_pos is None:\n",
        "                 reward = 100 # Reward for filling the board (optional, adjust as needed)\n",
        "\n",
        "\n",
        "        # Return the next state index directly\n",
        "        return self._get_state_index(), reward, self.game_over, {}\n",
        "\n",
        "    def _generate_fruit(self):\n",
        "        # Generate a new fruit position in an empty cell\n",
        "        all_cells = set((r, c) for r in range(self.board_size) for c in range(self.board_size))\n",
        "        snake_cells = set(self.snake)\n",
        "        empty_cells = list(all_cells - snake_cells)\n",
        "\n",
        "        if not empty_cells:\n",
        "            return None  # No empty cells\n",
        "        pos = random.choice(empty_cells)\n",
        "        self.board[pos] = 3\n",
        "        return pos\n",
        "\n",
        "    def _is_collision(self, head):\n",
        "        # Check for collisions with walls or self\n",
        "        r, c = head\n",
        "        # Wall collision\n",
        "        if r < 0 or r >= self.board_size or c < 0 or c >= self.board_size:\n",
        "            return True\n",
        "        # Self collision (check if head is in the body, excluding the potential next head position if it's the only element)\n",
        "        # Need to be careful here: the head is not yet added to the deque when checking for self-collision.\n",
        "        # So, check if the proposed next_head is in the current snake body.\n",
        "        if head in self.snake:  # This checks if the next head position is already occupied by the current snake body\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _calculate_reward(self, prev_dist, current_dist, ate_fruit, collision):\n",
        "        # Calculate the reward based on the game state\n",
        "        if collision:\n",
        "            return -10\n",
        "        elif ate_fruit:\n",
        "            return 10\n",
        "        elif current_dist < prev_dist:\n",
        "            return 1  # Moving closer to fruit\n",
        "        elif current_dist > prev_dist:\n",
        "            return -1  # Moving away from fruit\n",
        "        else:\n",
        "            return 0  # No change in distance\n",
        "\n",
        "    def _get_state_index(self):\n",
        "        \"\"\"\n",
        "        Calculates a unique index for the current state based on:\n",
        "        - Fruit position (relative to the top-left corner of the board)\n",
        "        - Snake head position (relative to the top-left corner of the board)\n",
        "        - Presence of snake body segments in the 8 directions around the snake head.\n",
        "\n",
        "        Returns:\n",
        "            A unique integer index representing the state.\n",
        "        \"\"\"\n",
        "        # Ensure snake is not empty\n",
        "        if not self.snake:\n",
        "            # This case should ideally not happen in a valid game state,\n",
        "            # but handle it to avoid errors. Return a default index.\n",
        "            return 0\n",
        "\n",
        "        head_r, head_c = self.snake[0]\n",
        "\n",
        "        # Handle case where fruit is None (board is full)\n",
        "        if self.fruit_pos is None:\n",
        "             # Define a specific state index for the 'board full' state\n",
        "             # This index should be outside the range of indices when fruit is present.\n",
        "             # We can use the maximum possible index from the case where fruit is present + 1\n",
        "             # Max index with fruit: (board_size * board_size) * (board_size * board_size) * (2**8) - 1\n",
        "             # So, board full index could be (board_size * board_size) * (board_size * board_size) * (2**8)\n",
        "             return (self.board_size * self.board_size) * (self.board_size * self.board_size) * (2**8)\n",
        "\n",
        "\n",
        "        # 1. Encode Fruit Position (16 possibilities for a 4x4 board)\n",
        "        fruit_index = self.fruit_pos[0] * self.board_size + self.fruit_pos[1]\n",
        "\n",
        "        # 2. Encode Head Position (16 possibilities - using absolute position)\n",
        "        head_index = head_r * self.board_size + head_c\n",
        "\n",
        "        # Base index combining fruit and head positions\n",
        "        # Total states so far: 16 * 16\n",
        "        base_index = fruit_index * (self.board_size * self.board_size) + head_index\n",
        "\n",
        "\n",
        "        # 3. Encode Presence of Snake Body Segments in 8 Directions around the head\n",
        "        # Directions: Up, Up-Right, Right, Down-Right, Down, Down-Left, Left, Up-Left\n",
        "        directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
        "        obstacle_bits = 0 # Use a bitmask to encode the 8 directions\n",
        "\n",
        "        for i, (dr, dc) in enumerate(directions):\n",
        "            check_r, check_c = head_r + dr, head_c + dc\n",
        "\n",
        "            # Check for wall collision\n",
        "            if check_r < 0 or check_r >= self.board_size or check_c < 0 or check_c >= self.board_size:\n",
        "                obstacle_bits |= (1 << i) # Set the bit if there's a wall\n",
        "\n",
        "            # Check for self collision (excluding the head itself)\n",
        "            # Check if the position is occupied by any snake segment EXCEPT the head\n",
        "            elif (check_r, check_c) in list(self.snake)[1:]: # Check if the position is in the snake's body (excluding head)\n",
        "                 obstacle_bits |= (1 << i) # Set the bit if there's a snake body segment\n",
        "\n",
        "        # Combine the base index with the obstacle bits\n",
        "        # Total states: (16 * 16) * 2^8\n",
        "        state_index = base_index * (2**8) + obstacle_bits\n",
        "\n",
        "        return state_index\n",
        "\n",
        "# Calculate the state space size based on the new representation\n",
        "# Fruit positions * Head positions * 8-directional obstacle bits + 1 (for board full state)\n",
        "state_space_size = (board_size * board_size) * (board_size * board_size) * (2**8) + 1\n",
        "action_space_size = 4 # Up, Down, Left, Right\n",
        "\n",
        "# Initialize the Q-table with zeros\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "\n",
        "print(f\"New state space size: {state_space_size}\")\n",
        "print(f\"Q-table shape: {q_table.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44a8fa0e"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the file path for saving/loading the Q-table\n",
        "q_table_file_path = '/content/drive/MyDrive/Colab Notebooks/snake_by_Qlearning1/snake_by_Qlearning_more_state_Qtable.npy'\n",
        "\n",
        "def save_q_table(q_table, file_path):\n",
        "    \"\"\"Saves the Q-table to a file using numpy's .npy format.\"\"\"\n",
        "    # Ensure the directory exists before saving\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "    np.save(file_path, q_table)\n",
        "    print(f\"Q-table saved to {file_path}\")\n",
        "\n",
        "def load_q_table(file_path):\n",
        "    \"\"\"Loads the Q-table from a file using numpy's .npy format.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Loading Q-table from {file_path}\")\n",
        "        return np.load(file_path)\n",
        "    else:\n",
        "        print(f\"No saved Q-table found at {file_path}. Starting with a new Q-table.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size):\n",
        "    \"\"\"\n",
        "    Implements the epsilon-greedy policy to select an action.\n",
        "\n",
        "    Args:\n",
        "        q_table (np.ndarray): The Q-table.\n",
        "        state_index (int): The index of the current state.\n",
        "        epsilon (float): The exploration rate.\n",
        "        action_space_size (int): The size of the action space.\n",
        "\n",
        "    Returns:\n",
        "        int: The selected action.\n",
        "    \"\"\"\n",
        "    # Explore: choose a random action\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.randrange(action_space_size)\n",
        "    # Exploit: choose the action with the highest Q-value for the current state\n",
        "    else:\n",
        "        return np.argmax(q_table[state_index, :])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei8aeOH1iqPc"
      },
      "source": [
        "# Q-learning parameters\n",
        "alpha = 0.2  # Learning rate (updated as requested)\n",
        "gamma = 0.6  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "max_epsilon = 1.0  # Exploration probability at start\n",
        "min_epsilon = 0.05 # Minimum exploration probability (updated as requested)\n",
        "epsilon_decay_rate = 0.999 # Exponential decay rate for epsilon\n",
        "\n",
        "num_episodes = 100000 # Number of training episodes (increased by 10 times)\n",
        "saving_frequency = 10000 # Save Q-table every 10,000 episodes\n",
        "\n",
        "# Create an instance of the environment\n",
        "env = SnakeEnv(board_size, max_steps)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    # Environment now returns the state index directly\n",
        "    state_index = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Select action using epsilon-greedy policy\n",
        "        action = epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size)\n",
        "\n",
        "        # Take action in the environment\n",
        "        # Environment now returns the next state index directly\n",
        "        next_state_index, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q-value using the Q-learning formula\n",
        "        # Q(s, a) = Q(s, a) + alpha * [reward + gamma * max(Q(s', a')) - Q(s, a)]\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table[next_state_index, :])\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "            # Update current state\n",
        "            state_index = next_state_index\n",
        "        else:\n",
        "            # If done, just update the Q-value for the terminal state transition\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward - current_q) # No future Q-value for terminal state\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "\n",
        "    # Save Q-table periodically\n",
        "    if (episode + 1) % saving_frequency == 0:\n",
        "        save_q_table(q_table, q_table_file_path)\n",
        "        print(f\"Saved Q-table at episode {episode + 1}\")\n",
        "\n",
        "\n",
        "    if (episode + 1) % 1000 == 0: # Print progress less frequently for more episodes\n",
        "        print(f\"Episode {episode + 1}/{num_episodes} completed. Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a13dc9ee"
      },
      "source": [
        "# Continuous Training Loop (Self-contained)\n",
        "\n",
        "# Define Q-learning Parameters\n",
        "alpha = 0.2  # Learning rate\n",
        "gamma = 0.6  # Discount factor\n",
        "# The fixed minimum epsilon for continuous training\n",
        "FIXED_MIN_EPSILON = 0.05 # Hardcoded minimum epsilon for continuous runs\n",
        "\n",
        "# Define Training Run Parameters for this continuous training loop\n",
        "continuous_run_episodes = 100000 # Train for 100,000 episodes per execution\n",
        "continuous_saving_frequency = 10000 # Save Q-table every 10,000 episodes\n",
        "\n",
        "# Define the file path for saving/loading the Q-table (Ensure this matches your desired path)\n",
        "\n",
        "# Load the Q-table if it exists\n",
        "# Assuming save_q_table and load_q_table functions are defined elsewhere and accessible\n",
        "# Assuming state_space_size and action_space_size are defined elsewhere and accessible (e.g., in the environment cell)\n",
        "loaded_q_table = load_q_table(q_table_file_path)\n",
        "\n",
        "# If a Q-table was loaded, use it. Otherwise, initialize a new one.\n",
        "if loaded_q_table is not None:\n",
        "    q_table = loaded_q_table\n",
        "    print(\"Continuing training from loaded Q-table.\")\n",
        "    # As requested, set epsilon to the fixed minimum value when continuing training\n",
        "    epsilon = FIXED_MIN_EPSILON\n",
        "    print(f\"Starting continuous training with epsilon: {epsilon:.2f}\")\n",
        "else:\n",
        "    # This case handles the very first execution if no Q-table exists\n",
        "    print(\"No saved Q-table found. Starting new training run with fixed minimum epsilon.\")\n",
        "    # Initialize a new Q-table (Requires state_space_size and action_space_size to be defined globally)\n",
        "    q_table = np.zeros((state_space_size, action_space_size))\n",
        "    # Start with the fixed minimum epsilon even for a fresh start in continuous mode\n",
        "    epsilon = FIXED_MIN_EPSILON\n",
        "\n",
        "\n",
        "# Create a new instance of the environment for this training run\n",
        "# Requires SnakeEnv, board_size, and max_steps to be defined globally\n",
        "env = SnakeEnv(board_size, max_steps)\n",
        "\n",
        "# Training loop for continuous execution\n",
        "for episode_in_run in range(continuous_run_episodes):\n",
        "    # Environment now returns the state index directly\n",
        "    state_index = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Select action using epsilon-greedy policy\n",
        "        # Use the fixed minimum epsilon\n",
        "        # Requires epsilon_greedy_policy and action_space_size to be defined globally\n",
        "        action = epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size)\n",
        "\n",
        "        # Take action in the environment\n",
        "        # Environment now returns the next state index directly\n",
        "        next_state_index, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q-value using the Q-learning formula\n",
        "        # Requires alpha and gamma to be defined globally\n",
        "        if not done:\n",
        "            # Ensure next_state_index is within bounds before accessing q_table\n",
        "            # Requires state_space_size to be defined globally\n",
        "            if next_state_index < 0 or next_state_index >= state_space_size:\n",
        "                print(f\"Error: Next state index {next_state_index} out of bounds.\")\n",
        "                # Handle error - perhaps break the loop or skip update\n",
        "                break # Exit inner while loop\n",
        "\n",
        "            max_future_q = np.max(q_table[next_state_index, :])\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "            # Update current state\n",
        "            state_index = next_state_index\n",
        "        else:\n",
        "            # If done, just update the Q-value for the terminal state transition\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward - current_q) # No future Q-value for terminal state\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "\n",
        "    # No epsilon decay needed as epsilon is fixed at the minimum\n",
        "\n",
        "\n",
        "    # Save Q-table periodically during this continuous run\n",
        "    # Requires save_q_table and continuous_saving_frequency to be defined globally\n",
        "    if (episode_in_run + 1) % continuous_saving_frequency == 0:\n",
        "        save_q_table(q_table, q_table_file_path)\n",
        "        print(f\"Saved Q-table during continuous run at episode {episode_in_run + 1}\")\n",
        "\n",
        "\n",
        "    if (episode_in_run + 1) % 1000 == 0: # Print progress less frequently\n",
        "        print(f\"Continuous Run Episode {episode_in_run + 1}/{continuous_run_episodes} completed.\")\n",
        "\n",
        "# Save Q-table at the end of this continuous run\n",
        "# Requires save_q_table and q_table_file_path to be defined globally\n",
        "save_q_table(q_table, q_table_file_path)\n",
        "print(\"Continuous training run finished. Q-table saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5b3107"
      },
      "source": [
        "# Evaluate Trained AI (Greedy Policy)\n",
        "\n",
        "# Ensure necessary variables and functions are accessible:\n",
        "# SnakeEnv, load_q_table, q_table_file_path, state_space_size, action_space_size\n",
        "# get_state_index (defined within SnakeEnv class now)\n",
        "# board_size, max_steps\n",
        "\n",
        "# Define the number of evaluation episodes\n",
        "num_eval_episodes = 5 # Run 5 evaluation episodes\n",
        "\n",
        "# Load the trained Q-table\n",
        "loaded_q_table = load_q_table(q_table_file_path)\n",
        "\n",
        "if loaded_q_table is None:\n",
        "    print(\"Error: No trained Q-table found. Please train the agent first.\")\n",
        "else:\n",
        "    q_table = loaded_q_table\n",
        "    print(\"Loaded trained Q-table for evaluation.\")\n",
        "\n",
        "    # Create an instance of the environment\n",
        "    env = SnakeEnv(board_size, max_steps)\n",
        "\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    fruits_eaten = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for episode in range(num_eval_episodes):\n",
        "        print(f\"\\n--- Evaluation Episode {episode + 1}/{num_eval_episodes} ---\")\n",
        "        state_index = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        # Simple text-based visualization function using tabs and different characters\n",
        "        def print_board(snake, fruit_pos, board_size):\n",
        "            # Create a board representation with characters\n",
        "            board_chars = [[' ' for _ in range(board_size)] for _ in range(board_size)]\n",
        "\n",
        "            # Mark snake\n",
        "            for i, (r, c) in enumerate(snake):\n",
        "                if 0 <= r < board_size and 0 <= c < board_size:\n",
        "                    board_chars[r][c] = 'H' if i == 0 else 'B' # H for Head, B for Body\n",
        "\n",
        "            # Mark fruit\n",
        "            if fruit_pos and 0 <= fruit_pos[0] < board_size and 0 <= fruit_pos[1] < board_size:\n",
        "                 board_chars[fruit_pos[0]][fruit_pos[1]] = 'F' # F for Fruit\n",
        "\n",
        "            # Print the board using tabs and newlines\n",
        "            for r in range(board_size):\n",
        "                print(\"\\t\".join(board_chars[r]))\n",
        "            print(\"-\" * (board_size * 4)) # Separator line\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            # Select action using greedy policy (epsilon = 0)\n",
        "            # Select the action with the highest Q-value for the current state\n",
        "            if state_index < 0 or state_index >= state_space_size:\n",
        "                print(f\"Error: State index {state_index} out of bounds during evaluation.\")\n",
        "                # Break if state index is invalid\n",
        "                break\n",
        "\n",
        "            # Use np.argmax to get the action with the highest Q-value\n",
        "            action = np.argmax(q_table[state_index, :])\n",
        "\n",
        "            # Print current state and chosen action (optional, for debugging)\n",
        "            # print(f\"State Index: {state_index}, Chosen Action: {action}\")\n",
        "\n",
        "            # Visualize current board before the step\n",
        "            print(f\"Step: {episode_steps}, Action: {action}\")\n",
        "            print_board(env.snake, env.fruit_pos, env.board_size)\n",
        "\n",
        "\n",
        "            # Take action in the environment\n",
        "            # env.step now returns the next state index directly\n",
        "            next_state_index, reward, done, info = env.step(action)\n",
        "\n",
        "            # Accumulate reward and steps\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Update current state index\n",
        "            state_index = next_state_index\n",
        "\n",
        "            # You can add a small delay here to slow down the visualization\n",
        "            # import time\n",
        "            # time.sleep(0.1)\n",
        "\n",
        "        # Game Over for this episode\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(episode_steps)\n",
        "        # Calculate fruits eaten: Initial length is 1, each fruit adds 1 to length\n",
        "        fruits_eaten.append(len(env.snake) - 1)\n",
        "\n",
        "\n",
        "        print(f\"Episode finished. Reward: {episode_reward}, Steps: {episode_steps}, Snake Length: {len(env.snake)}\")\n",
        "        # Visualize the final board state\n",
        "        print(\"Final Board State:\")\n",
        "        print_board(env.snake, env.fruit_pos, env.board_size)\n",
        "\n",
        "\n",
        "    # Print average performance\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    avg_steps = np.mean(total_steps)\n",
        "    avg_fruits_eaten = np.mean(fruits_eaten)\n",
        "\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"Average Reward over {num_eval_episodes} episodes: {avg_reward:.2f}\")\n",
        "    print(f\"Average Steps per episode: {avg_steps:.2f}\")\n",
        "    print(f\"Average Fruits eaten per episode: {avg_fruits_eaten:.2f}\")\n",
        "\n",
        "    # You can also add a more detailed visualization here if needed,\n",
        "    # perhaps rendering the game board state at each step."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
