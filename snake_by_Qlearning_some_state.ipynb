{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbd2163"
      },
      "source": [
        "%pip install tensorflow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8a6f6b6"
      },
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "board_size=4\n",
        "max_steps=2*board_size*board_size\n",
        "\n",
        "class SnakeEnv:\n",
        "    def __init__(self,board_size , max_steps):\n",
        "        self.board_size=board_size\n",
        "        self.max_steps=max_steps\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board= np.zeros((board_size, board_size), dtype=int) # Add board back\n",
        "        self.snake = collections.deque([(0, 0)])\n",
        "        self.fruit_pos = self._generate_fruit()\n",
        "        self.steps = 0\n",
        "        self.game_over = False\n",
        "        self.board[self.snake[0]] = 1\n",
        "        return self.board\n",
        "    def step(self, action):\n",
        "        if self.game_over:\n",
        "            return self.board, 0, True, {}\n",
        "        self.steps += 1\n",
        "        head_r, head_c = self.snake[0]\n",
        "        if action == 0: # Up\n",
        "            next_head = (head_r - 1, head_c)\n",
        "        elif action == 1: # Down\n",
        "            next_head = (head_r + 1, head_c)\n",
        "        elif action == 2: # Left\n",
        "            next_head = (head_r, head_c - 1)\n",
        "        elif action == 3: # Right\n",
        "            next_head = (head_r, head_c + 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "        collision = self._is_collision(next_head)\n",
        "\n",
        "        if collision or self.steps >= self.max_steps:\n",
        "            self.game_over = True\n",
        "            reward = -10 # Collision or max steps reached penalty\n",
        "            return self.board, reward, self.game_over, {}\n",
        "\n",
        "        prev_dist = abs(head_r - self.fruit_pos[0]) + abs(head_c - self.fruit_pos[1]) if self.fruit_pos else 0\n",
        "        ate_fruit = False\n",
        "        self.board[self.snake[0]] =2\n",
        "        self.board[next_head] = 1 # Add new head\n",
        "        self.snake.appendleft(next_head)\n",
        "        if next_head == self.fruit_pos:\n",
        "            ate_fruit = True\n",
        "            self.fruit_pos = self._generate_fruit()\n",
        "            self.steps = 0 # Reset steps on eating fruit\n",
        "        else:\n",
        "            self.board[self.snake[-1]] = 0 # Remove tail\n",
        "            self.snake.pop() # Remove tail\n",
        "        current_dist = abs(next_head[0] - self.fruit_pos[0]) + abs(next_head[1] - self.fruit_pos[1]) if self.fruit_pos else 0\n",
        "        reward = self._calculate_reward(prev_dist, current_dist, ate_fruit)\n",
        "        return self.board, reward, self.game_over, {}\n",
        "    def _generate_fruit(self):\n",
        "        # Generate a new fruit position in an empty cell\n",
        "        all_cells = set((r, c) for r in range(self.board_size) for c in range(self.board_size))\n",
        "        snake_cells = set(self.snake)\n",
        "        empty_cells = list(all_cells - snake_cells)\n",
        "        if not empty_cells:\n",
        "            return None  # No empty cells\n",
        "        pos=random.choice(empty_cells)\n",
        "        self.board[pos]=3\n",
        "        return pos\n",
        "    def _is_collision(self, head):\n",
        "        r, c = head\n",
        "        if r < 0 or r >= self.board_size or c < 0 or c >= self.board_size:\n",
        "            return True\n",
        "        if head in self.snake:\n",
        "             return True\n",
        "        return False\n",
        "    def _calculate_reward(self, prev_dist, current_dist, ate_fruit):\n",
        "        if ate_fruit:\n",
        "            return 10\n",
        "        elif current_dist < prev_dist:\n",
        "            return 1  # Moving closer to fruit\n",
        "        elif current_dist > prev_dist:\n",
        "            return -1  # Moving away from fruit\n",
        "        else:\n",
        "            return 0 # No change in distance\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fdcb0a3",
        "outputId": "92c32b38-6208-4f86-94fa-8d87a8bf461d"
      },
      "source": [
        "\n",
        "state_space_size = board_size * board_size * board_size * board_size # (fruit_r, fruit_c, head_r, head_c)\n",
        "action_space_size = 4 # (Up, Down, Left, Right)\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "\n",
        "print(\"Q-table shape:\", q_table.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table shape: (256, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb0fb580"
      },
      "source": [
        "def get_state_index(fruit_pos, snake, board_size):\n",
        "    \"\"\"\n",
        "    Converts the game state into a unique integer index.\n",
        "\n",
        "    Args:\n",
        "        fruit_pos: A tuple (row, col) of the fruit position.\n",
        "        snake: A collections.deque of tuples representing the snake's body.\n",
        "        board_size: The size of the square game board.\n",
        "\n",
        "    Returns:\n",
        "        A unique integer index representing the state.\n",
        "    \"\"\"\n",
        "    head_r, head_c = snake[0]\n",
        "    fruit_r, fruit_c = fruit_pos\n",
        "\n",
        "    # Calculate a unique index based on the positions\n",
        "    # Index = fruit_r * board_size^3 + fruit_c * board_size^2 + head_r * board_size^1 + head_c * board_size^0\n",
        "    state_index = fruit_r * (board_size ** 3) + fruit_c * (board_size ** 2) + head_r * board_size + head_c\n",
        "    return state_index\n",
        "\n",
        "# Example usage:\n",
        "# Assuming env is an instance of SnakeEnv\n",
        "# state_index = get_state_index(env.fruit_pos, env.snake, env.board_size)\n",
        "# print(f\"State index: {state_index}\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19b2c723"
      },
      "source": [
        "def epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size):\n",
        "    \"\"\"\n",
        "    Selects an action based on the epsilon-greedy policy.\n",
        "\n",
        "    Args:\n",
        "        q_table: The Q-table.\n",
        "        state_index: The index of the current state.\n",
        "        epsilon: The epsilon value for exploration.\n",
        "        action_space_size: The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        The selected action (an integer).\n",
        "    \"\"\"\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Explore: Choose a random action\n",
        "        action = random.randrange(action_space_size)\n",
        "    else:\n",
        "        # Exploit: Choose the action with the highest Q-value for the current state\n",
        "        action = np.argmax(q_table[state_index, :])\n",
        "    return action\n",
        "\n",
        "# Example usage (assuming q_table, state_index, epsilon, and action_space_size are defined)\n",
        "# selected_action = epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size)\n",
        "# print(f\"Selected action: {selected_action}\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei8aeOH1iqPc"
      },
      "source": [
        "# Q-learning parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.6  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "max_epsilon = 1.0  # Exploration probability at start\n",
        "min_epsilon = 0.01 # Minimum exploration probability\n",
        "epsilon_decay_rate = 0.999 # Exponential decay rate for epsilon\n",
        "num_episodes = 100000 # Number of training episodes (increased by 10 times)\n",
        "# Create an instance of the environment\n",
        "env = SnakeEnv(board_size, max_steps)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state_index = get_state_index(env.fruit_pos, env.snake, env.board_size)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Select action using epsilon-greedy policy\n",
        "        action = epsilon_greedy_policy(q_table, state_index, epsilon, action_space_size)\n",
        "\n",
        "        # Take action in the environment\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Check if fruit_pos is None after the step (board full)\n",
        "        if env.fruit_pos is None:\n",
        "            done = True # Treat board full as a terminal state\n",
        "            next_state_index = state_space_size - 1\n",
        "            pass # The Q-table update should only happen if not done\n",
        "\n",
        "        if not done:\n",
        "            # Get index of the next state\n",
        "            next_state_index = get_state_index(env.fruit_pos, env.snake, env.board_size)\n",
        "\n",
        "            # Update Q-value using the Q-learning formula\n",
        "            # Q(s, a) = Q(s, a) + alpha * [reward + gamma * max(Q(s', a')) - Q(s, a)]\n",
        "            max_future_q = np.max(q_table[next_state_index, :])\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "            # Update current state\n",
        "            state_index = next_state_index\n",
        "        else:\n",
        "            # If done, just update the Q-value for the terminal state transition\n",
        "            current_q = q_table[state_index, action]\n",
        "            new_q = current_q + alpha * (reward - current_q) # No future Q-value for terminal state\n",
        "            q_table[state_index, action] = new_q\n",
        "\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "\n",
        "    if (episode + 1) % 1000 == 0: # Print progress less frequently for more episodes\n",
        "        print(f\"Episode {episode + 1}/{num_episodes} completed. Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7630e5a3"
      },
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def evaluate_agent(q_table, env, num_eval_episodes, visualize=False, delay=0.1):\n",
        "    episode_rewards = []\n",
        "    fruits_eaten_per_episode = []\n",
        "    final_snake_length_per_episode = []\n",
        "\n",
        "    print(f\"\\nStarting evaluation for {num_eval_episodes} episodes...\")\n",
        "\n",
        "    for episode in range(num_eval_episodes):\n",
        "        state = env.reset()\n",
        "        # Ensure fruit_pos is not None after reset before getting state index\n",
        "        if env.fruit_pos is None:\n",
        "             # This case should ideally not happen immediately after reset,\n",
        "             # but handling it for robustness.\n",
        "             print(f\"Warning: Fruit is None after reset in evaluation episode {episode + 1}\")\n",
        "             episode_rewards.append(0) # Or some other appropriate reward\n",
        "             fruits_eaten_per_episode.append(0)\n",
        "             final_snake_length_per_episode.append(len(env.snake))\n",
        "             continue # Skip this episode\n",
        "\n",
        "        state_index = get_state_index(env.fruit_pos, env.snake, env.board_size)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        fruits_eaten = 0\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            if visualize:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Evaluation Episode {episode + 1}/{num_eval_episodes}\")\n",
        "                print(\"Current State:\")\n",
        "                # Assuming the state returned by env.step and env.reset is the board\n",
        "                # Print the board using characters instead of numbers\n",
        "                for r in range(env.board_size):\n",
        "                    row_display = \"\"\n",
        "                    for c in range(env.board_size):\n",
        "                        if (r, c) == env.snake[0]:\n",
        "                            row_display += \" H \" # Snake head\n",
        "                        elif (r, c) in list(env.snake)[1:]:\n",
        "                            row_display += \" S \" # Snake body\n",
        "                        elif (r, c) == env.fruit_pos:\n",
        "                            row_display += \" F \" # Fruit\n",
        "                        else:\n",
        "                            row_display += \" . \" # Empty cell\n",
        "                    print(row_display)\n",
        "\n",
        "                print(f\"Total Reward: {total_reward}\")\n",
        "                print(f\"Fruits eaten: {fruits_eaten}\")\n",
        "                print(f\"Snake length: {len(env.snake)}\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "            # Select the best action based on the Q-table (exploitation only)\n",
        "            action = np.argmax(q_table[state_index, :])\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Check if fruit was eaten in this step\n",
        "            if reward == 10: # Assuming reward of 10 is only for eating fruit\n",
        "                 fruits_eaten += 1\n",
        "\n",
        "\n",
        "            # Update state index for the next step if not done\n",
        "            if not done:\n",
        "                 # Ensure fruit_pos is not None before getting state index\n",
        "                if env.fruit_pos is not None:\n",
        "                    state_index = get_state_index(env.fruit_pos, env.snake, env.board_size)\n",
        "                else:\n",
        "                    # Handle case where fruit_pos is None during gameplay (board full)\n",
        "                    # This is likely a terminal state, so state_index won't be used for Q-lookup\n",
        "                    done = True # Ensure the loop terminates\n",
        "                    print(f\"Evaluation Episode {episode + 1}/{num_eval_episodes} ended: Board Full\")\n",
        "\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        fruits_eaten_per_episode.append(fruits_eaten)\n",
        "        final_snake_length_per_episode.append(len(env.snake))\n",
        "\n",
        "        if visualize:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Evaluation Episode {episode + 1}/{num_eval_episodes} finished with total reward: {total_reward}, fruits eaten: {fruits_eaten}, final length: {len(env.snake)}\")\n",
        "            print(\"Final State:\")\n",
        "            # Print the final board state using characters\n",
        "            for r in range(env.board_size):\n",
        "                row_display = \"\"\n",
        "                for c in range(env.board_size):\n",
        "                    if (r, c) == env.snake[0]:\n",
        "                        row_display += \" H \" # Snake head\n",
        "                    elif (r, c) in list(env.snake)[1:]:\n",
        "                        row_display += \" S \" # Snake body\n",
        "                    elif (r, c) == env.fruit_pos:\n",
        "                        row_display += \" F \" # Fruit\n",
        "                    else:\n",
        "                        row_display += \" . \" # Empty cell\n",
        "                print(row_display)\n",
        "            print(f\"Final Total Reward: {total_reward}, Fruits eaten: {fruits_eaten}, Final length: {len(env.snake)}\")\n",
        "\n",
        "\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "    avg_fruits_eaten = np.mean(fruits_eaten_per_episode)\n",
        "    avg_final_snake_length = np.mean(final_snake_length_per_episode)\n",
        "\n",
        "\n",
        "    print(f\"\\nAverage reward over {num_eval_episodes} evaluation episodes: {avg_reward:.2f}\")\n",
        "    print(f\"Average fruits eaten per episode: {avg_fruits_eaten:.2f}\")\n",
        "    print(f\"Average final snake length per episode: {avg_final_snake_length:.2f}\")\n",
        "\n",
        "\n",
        "    return episode_rewards, fruits_eaten_per_episode, final_snake_length_per_episode\n",
        "\n",
        "# Example usage after training:\n",
        "# eval_rewards, eval_fruits, eval_lengths = evaluate_agent(q_table, env, num_eval_episodes=100, visualize=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc1e24cb"
      },
      "source": [
        "# Evaluate the trained agent with visualization\n",
        "num_eval_episodes = 100\n",
        "eval_rewards, eval_fruits, eval_lengths = evaluate_agent(q_table, env, num_eval_episodes, visualize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707914c3"
      },
      "source": [
        "### 儲存訓練好的 Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4e1e3cd",
        "outputId": "d4c950f8-ad9a-43ee-8b04-cd3175229195"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the path to save the q_table\n",
        "q_table_save_path = '/content/drive/MyDrive/Colab Notebooks/snake_by_Qlearning1/snake_by_Qlearning_some_state_table.npy' # You can change the filename and path\n",
        "\n",
        "# Save the q_table to a file\n",
        "np.save(q_table_save_path, q_table)\n",
        "\n",
        "print(f\"Q-table saved successfully to {q_table_save_path}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table saved successfully to /content/drive/MyDrive/Colab Notebooks/snake_by_Qlearning1/snake_by_Qlearning_some_state_table.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b027e1e2"
      },
      "source": [
        "### 載入儲存的 Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6c687a3",
        "outputId": "cb0bf148-24bb-4f44-e44c-af3b293d6e04"
      },
      "source": [
        "# Define the path to load the q_table from\n",
        "q_table_load_path = '/content/drive/MyDrive/Colab Notebooks/snake_by_Qlearning1/snake_by_Qlearning_some_state_table.npy' # Make sure this matches the save path\n",
        "\n",
        "# Load the q_table from the file\n",
        "try:\n",
        "    q_table = np.load(q_table_load_path)\n",
        "    print(f\"Q-table loaded successfully from {q_table_load_path}\")\n",
        "    print(\"Current Q-table shape:\", q_table.shape)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Q-table file not found at {q_table_load_path}\")\n",
        "    print(\"Please ensure the file exists or train a new Q-table.\")\n",
        "    # Optionally, initialize a new Q-table if the file is not found\n",
        "    # q_table = np.zeros((state_space_size, action_space_size))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table loaded successfully from /content/drive/MyDrive/Colab Notebooks/snake_by_Qlearning1/snake_by_Qlearning_some_state_table.npy\n",
            "Current Q-table shape: (256, 4)\n"
          ]
        }
      ]
    }
  ]
}
